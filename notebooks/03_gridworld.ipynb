{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridworld\n",
    "\n",
    "[Exercises](https://github.com/SUTD-MARL/reinforcement-learning-exercises/tree/master/DP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, sys\n",
    "import pprint\n",
    "\n",
    "sys.path.insert(\n",
    "    1,\n",
    "    os.path.realpath(\n",
    "        os.path.join(os.path.pardir, \"reinforcement-learning-exercises\")\n",
    "    )\n",
    ")\n",
    "\n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "pp = pprint.PrettyPrinter(indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ 0: { 0: [(1.0, 0, 0.0, True)],\n",
      "       1: [(1.0, 0, 0.0, True)],\n",
      "       2: [(1.0, 0, 0.0, True)],\n",
      "       3: [(1.0, 0, 0.0, True)]},\n",
      "  1: { 0: [(1.0, 1, -1.0, False)],\n",
      "       1: [(1.0, 2, -1.0, False)],\n",
      "       2: [(1.0, 5, -1.0, False)],\n",
      "       3: [(1.0, 0, -1.0, True)]},\n",
      "  2: { 0: [(1.0, 2, -1.0, False)],\n",
      "       1: [(1.0, 3, -1.0, False)],\n",
      "       2: [(1.0, 6, -1.0, False)],\n",
      "       3: [(1.0, 1, -1.0, False)]},\n",
      "  3: { 0: [(1.0, 3, -1.0, False)],\n",
      "       1: [(1.0, 3, -1.0, False)],\n",
      "       2: [(1.0, 7, -1.0, False)],\n",
      "       3: [(1.0, 2, -1.0, False)]},\n",
      "  4: { 0: [(1.0, 0, -1.0, True)],\n",
      "       1: [(1.0, 5, -1.0, False)],\n",
      "       2: [(1.0, 8, -1.0, False)],\n",
      "       3: [(1.0, 4, -1.0, False)]},\n",
      "  5: { 0: [(1.0, 1, -1.0, False)],\n",
      "       1: [(1.0, 6, -1.0, False)],\n",
      "       2: [(1.0, 9, -1.0, False)],\n",
      "       3: [(1.0, 4, -1.0, False)]},\n",
      "  6: { 0: [(1.0, 2, -1.0, False)],\n",
      "       1: [(1.0, 7, -1.0, False)],\n",
      "       2: [(1.0, 10, -1.0, False)],\n",
      "       3: [(1.0, 5, -1.0, False)]},\n",
      "  7: { 0: [(1.0, 3, -1.0, False)],\n",
      "       1: [(1.0, 7, -1.0, False)],\n",
      "       2: [(1.0, 11, -1.0, False)],\n",
      "       3: [(1.0, 6, -1.0, False)]},\n",
      "  8: { 0: [(1.0, 4, -1.0, False)],\n",
      "       1: [(1.0, 9, -1.0, False)],\n",
      "       2: [(1.0, 12, -1.0, False)],\n",
      "       3: [(1.0, 8, -1.0, False)]},\n",
      "  9: { 0: [(1.0, 5, -1.0, False)],\n",
      "       1: [(1.0, 10, -1.0, False)],\n",
      "       2: [(1.0, 13, -1.0, False)],\n",
      "       3: [(1.0, 8, -1.0, False)]},\n",
      "  10: { 0: [(1.0, 6, -1.0, False)],\n",
      "        1: [(1.0, 11, -1.0, False)],\n",
      "        2: [(1.0, 14, -1.0, False)],\n",
      "        3: [(1.0, 9, -1.0, False)]},\n",
      "  11: { 0: [(1.0, 7, -1.0, False)],\n",
      "        1: [(1.0, 11, -1.0, False)],\n",
      "        2: [(1.0, 15, -1.0, True)],\n",
      "        3: [(1.0, 10, -1.0, False)]},\n",
      "  12: { 0: [(1.0, 8, -1.0, False)],\n",
      "        1: [(1.0, 13, -1.0, False)],\n",
      "        2: [(1.0, 12, -1.0, False)],\n",
      "        3: [(1.0, 12, -1.0, False)]},\n",
      "  13: { 0: [(1.0, 9, -1.0, False)],\n",
      "        1: [(1.0, 14, -1.0, False)],\n",
      "        2: [(1.0, 13, -1.0, False)],\n",
      "        3: [(1.0, 12, -1.0, False)]},\n",
      "  14: { 0: [(1.0, 10, -1.0, False)],\n",
      "        1: [(1.0, 15, -1.0, True)],\n",
      "        2: [(1.0, 14, -1.0, False)],\n",
      "        3: [(1.0, 13, -1.0, False)]},\n",
      "  15: { 0: [(1.0, 15, 0.0, True)],\n",
      "        1: [(1.0, 15, 0.0, True)],\n",
      "        2: [(1.0, 15, 0.0, True)],\n",
      "        3: [(1.0, 15, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(env.P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policy evaluation](https://github.com/SUTD-MARL/reinforcement-learning-exercises/blob/master/DP/Policy%20Evaluation.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_fn(env, value, state, discount_factor):\n",
    "    actions = env.P[state]\n",
    "    return [\n",
    "        transitions[0][2] + discount_factor * sum([\n",
    "            transition[0] * value[transition[1]] for transition in transitions\n",
    "        ]) for a, transitions in actions.items()\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval(policy, env, discount_factor=1.0,\n",
    "                theta=0.00001):\n",
    "    V = np.zeros(env.nS)\n",
    "    new_V = np.zeros(env.nS)\n",
    "    distance = 1\n",
    "    \n",
    "    while distance > theta:\n",
    "        for s in range(env.nS):\n",
    "            actions = env.P[s]\n",
    "            q = q_fn(env, V, s, discount_factor)\n",
    "            new_V[s] = sum([\n",
    "                policy[s, a] * q[a] for a in actions.keys()\n",
    "            ])\n",
    "        \n",
    "        distance = np.linalg.norm(V - new_V)\n",
    "        V = new_V.copy()        \n",
    "        \n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "v = policy_eval(random_policy, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -13.9999661 , -19.99994976, -21.99994378],\n",
       "       [-13.9999661 , -17.99995574, -19.9999501 , -19.99994976],\n",
       "       [-19.99994976, -19.9999501 , -17.99995574, -13.9999661 ],\n",
       "       [-21.99994378, -19.99994976, -13.9999661 ,   0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(v, (4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policy iteration](https://github.com/SUTD-MARL/reinforcement-learning-exercises/blob/master/DP/Policy%20Iteration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, policy_eval_fn=policy_eval, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Policy Improvement Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_fn: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    new_policy = np.zeros([env.nS, env.nA])\n",
    "    distance = 1\n",
    "    \n",
    "    while distance > theta:\n",
    "        V = policy_eval_fn(policy, env, discount_factor=discount_factor, theta=theta)\n",
    "        for s in range(env.nS):\n",
    "            actions = env.P[s]\n",
    "            q = q_fn(env, V, s, discount_factor)\n",
    "            best_action = max(enumerate(q),key=lambda x: x[1])[0]\n",
    "            for a in actions.keys():\n",
    "                if a == best_action:\n",
    "                    new_policy[s, a] = 1\n",
    "                else:\n",
    "                    new_policy[s, a] = 0\n",
    "        \n",
    "        distance = np.linalg.norm(policy - new_policy)\n",
    "        policy = new_policy.copy()\n",
    "    \n",
    "    return policy, policy_eval_fn(policy, env, discount_factor=discount_factor, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, value = policy_improvement(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 2],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 1, 2],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(policy.argmax(axis=1), (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(value, (4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Value iteration](https://github.com/SUTD-MARL/reinforcement-learning-exercises/blob/master/DP/Value%20Iteration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    new_V = np.zeros(env.nS)\n",
    "    distance = 1\n",
    "    while distance > theta:\n",
    "        for s in range(env.nS):\n",
    "            actions = env.P[s]\n",
    "            q = q_fn(env, V, s, discount_factor)\n",
    "            best_action = max(enumerate(q),key=lambda x: x[1])[0]\n",
    "            new_V[s] = max(q)\n",
    "            for a in actions.keys():\n",
    "                if a == best_action:\n",
    "                    policy[s, a] = 1\n",
    "                else:\n",
    "                    policy[s, a] = 0\n",
    "    \n",
    "        distance = np.linalg.norm(V - new_V)\n",
    "        V = new_V.copy()\n",
    "        \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy, value = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3, 3, 2],\n",
       "       [0, 0, 0, 2],\n",
       "       [0, 0, 1, 2],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(policy.argmax(axis=1), (4,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0., -1., -2., -3.],\n",
       "       [-1., -2., -3., -2.],\n",
       "       [-2., -3., -2., -1.],\n",
       "       [-3., -2., -1.,  0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.reshape(value, (4,4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
